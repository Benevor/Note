# 实验室方向

分布式系统的构架，一般由底层的存储、计算、网络三种资源支撑。

## 存储

1. 存储引擎，聚焦在单节点存储引擎。

    - 在与华为的交流中发现，InfluxDB使用golang作为开发语言是一个错误的选择，golang底层的内存管理在边缘或者资源紧缺的节点上，难以避免内存膨胀。采用GC的语言估计都难以避免这个问题，包括Java？

    - 存储引擎是一个基础构件，在分布式存储和计算中，都需要由该部件构成。实验室缺乏存储引擎的实现，可以考虑从levelDB出发，构建一个基于协程的、分页的存储引擎。

2. SSD内嵌存储引擎。

    - 存储与计算分离是一个趋势，将存储与计算融合构成对象存储的方案估计会逐渐没落。在功能单一的存储节点上再构建分布式存储系统、分布式文件系统，是存算分离的方向。

    - SSD颗粒推出多年，未来可能会被PM取代，但以硬盘形式存在SSD器件应该不会没落。另一方面，传统SSD控制器的开发商应该在LSM上的积累比较少，这可能是在存储领域比较好的切入点。

    - SSD控制器本身就是一个微控制器，采用ARM、RISC-V来开发这样的控制器本身门槛比较低。将LSM结构下沉到SSD器件，在底层做存储优化，bypass操作系统内核，能够直接而方便的操纵存储颗粒，降低读写放大。

3. 基于PM的存储引擎。

    - PM器件在不久前商业推出，预计会对计算机系统结构造成比较大的冲击。内存即硬盘？

    - HDD/SSD器件都存在追加性能显著高于update-in-place的特性，由此，LSM结构成为云存储上的标准存储方案。但PM器件上，update-in-place虽然也慢于读，但这个差异必HDD/SSD上要小得多。问题：LSM结构是否适合作为PM存储？

    - 进一步，在PM器件上，什么数据结构更适合？btree、rbtree、hashtable？

    - 数据保存在PM上，以分页的形式访问？文件系统如何处理？系统自举后，文件系统如何自举？

4. 分布式存储系统。

    - 在存储引擎的基础上，构建分布式存储系统。

    - 数据的存储分为三类：a) update-in-place数据；b) append数据；c) 堆数据。update-in-place数据，包括索引等，append数据包括log等，堆数据可以指value数据。分为三类后，可以采用kv分离的方案，将索引做得很小。

    - update-in-place数据采用Paxos/Raft维护一致性，在checkpoint之前的log数据也采用Paxos/Raft维护。checkpoint之前的log直接使用mirror方式保存。

    - 堆数据的合并，有没有更好的方案？引入PM新器件、分页方案后能否进一步降低堆数据的merge开销？

5. 分布式内存系统。

    - 引入PM后，分布式存储可以在数据中心构建全局的分布式内存。后台存储节点通过rdma将数据快速映射到远端。分布式内存有助于简化分布式计算的设计，不采用消息作为通信介质，而是直接在共享内存模型上构建分布式算法。

    - 分布式内存要暴露出标准的内存操作接口：读、写、原子操作、加锁、内存屏障等。

    - 分布式内存底层的问题在于，如何维护数据的一致性。各客户节点访问的数据，如何保持彼此同步，保持与后台存储节点的同步？一种方案是采用事务型接口，以事务的方式去访问内存，但这与上层暴露的读写操作如何协调？

## 计算

    1. 流式计算。

    - 云计算最初采用以文件为单位的批处理方式来计算，抽象为MapReduce。批处理的主要问题在于落盘开销比较大，不利于实时处理。流计算采用流来连接分布式节点，计算完毕后直接以流的形式推送到下一个节点，性能比较高。

    - stream抽象，与批处理中的文件抽象对应。storm、spark、flink的流定义不尽相同，需要先抽象stream的主要操作到底有哪些？流批一体是方向，首先仍然是stream与文件的抽象如何等同？

    - 批处理算子实现，主要集中在SQL算子：单目操作、双目操作算子实现。多路归并？

    - 从流处理的基本思路来看，要避免落盘。一个问题是，如何维持流处理的状态？snapshot。快照保持了哪些状态？

    - 存储通过Paxos/Raft维持一致性，流处理中，分布式是计算流是否需要冗余？计算的冗余如何维护？

    - 在与华为的交流中，云计算中心汇集各种数据，一般是在云计算中心的边缘设置Kafka节点，该节点接收并保存数据，作为流处理的起点，sub/pub各种流处理。该节点设计上需要考虑什么？

    2. Paxos/Raft协议形式化分析

    - 与单机上的算法不同，分布式算法的执行充满了不确定性，消息的延迟、节点的崩溃等。Paxos/Raft算法本身就不好理解，更容易在实现上出现偏差。

    - 对Paxos/Raft等分布式协议做形式化分析、验证。

    - 针对特定场景，与上层应用结合，设计冗余存储算法，并验证。

    - 探索分布式算法转化为代码的更容易的方式，探索对代码进行底层验证和测试的方式。

    - availability在集群中是一个体系，凡是需要冗余都采用Paxos/Raft方案是一种浪费。要求在集群中设计一个root Paxos，其余冗余实例以该Paxos实例为根，简化Paxos设计。

    3. 全内存计算

    4. 异构计算节点

## 数据库

    1. 时序数据库

    2. 时空数据库

    - 时空数据库在社交媒体、情报分析领域应用广泛，在防疫、军事领域有较高的应用价值。

    - 时空数据库的数据模型？还需要考虑图的表示。

    - 构建思路与方案，简单的将时序数据库与空间数据库结合？以时序数据库的方案接入数据，将数据存储在图节点上，空间信息如何结合？作为属性？

    3. 图数据库

    - 分为图计算、图数据库两个方向。

    - 图计算主要是如何实现各种分布式算子，如何PageRank，最短路径等。目前的分布式图计算框架主要有两种：google的buckle，分布式锁，性能都不太好。一个思路是结合算子的特性，设计特别的计算构架。

    - 图的数据模型？

    - 图存储方案？按边切割，按节点切割。都不太好，这与应用相关。动态感知上层应用，调整图的存储方式？

    - 基于mobile agent的图计算方案？

    4. 分布式事务优化

    5. 执行计划的优化

    6. HTAP/ETL

## IoT

    1. 边缘计算容器

    - 5G的杀手应用估计出在IoT上，目前的IoT方案一般是，在边缘节点上设置容器，由容器实例实现各种IoT应用。边缘设备挂接终端设备，边缘IoT再连接云中心。边缘IoT设备是一个承上启下的节点。

    - 目前k8s、k3s、kubeedge都是采用golang实现的，从InfluxDB的经验来看，边缘设备上采用golang实现是一种错误。能否采用C++/Rust实现docker？



